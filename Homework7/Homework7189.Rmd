---
title: "Homework 7"
author: "Daria Barbour-Brown | Bailey Ho | Warren Kennedy"
date: "2023-05-16"
output:
  pdf_document: default
  html_document: default
---

# Conceptual Problem 

## Question 1

*Linear discriminant analysis and logistic regression can both be used for binary classification. How would you decide to use one method versus the other in practice?*

LDA assumes the data follows a multivariate normal distribution and equal class variances. If these assumptions are not met, LDA may not give accurate results. In contrast, logistic regression makes fewer assumptions about the data distribution, offering more flexibility when LDA assumptions are violated.

Logistic regression directly estimates class membership probabilities, allowing for the interpretation of predictor-outcome relationships. It provides odds ratios and p-values to assess the impact of individual predictors. On the other hand, LDA does not directly estimate these probabilities but focuses on class separation and provides functions for classifying new observations.

The choice between the methods also depends on the sample size. LDA performs well with large sample sizes relative to the number of predictors, as it more reliably estimates parameters. Conversely, logistic regression is better suited for smaller sample sizes and can handle a higher number of predictors. If you're dealing with high-dimensional data, where the number of predictors exceeds the number of observations, logistic regression is considered more appropriate.

To summarize, logistic regression is advantageous when LDA assumptions are violated, providing direct estimates of class probabilities and interpretability of predictor effects. LDA excels with large sample sizes relative to predictors, focusing on class separation rather than individual predictors. Consider the characteristics of your data and the specific analysis goals to guide your choice between LDA and logistic regression.

# Application Problems

## Question 2

### Part A

*Fit a logistic regression with diabetes (Y/N) as the response, and glucose levels as a covariate, as well as an indicator for an individual having a BMI greater than 35 (BMI is given by mass in this dataset). Print the summary of this regression object.*

  
```{r}
library(mlbench)
library(ggplot2)
library(dplyr)
data("PimaIndiansDiabetes") #loads data under "name"
pimaData <- PimaIndiansDiabetes
#create new numeric col of all 1s
pimaData$response <- 1
#crate response col, transform string to categorical 
pimaData$response[pimaData$diabetes == 'neg'] <- 0
#bmi > 35 col
pimaData$bmi <- 0
pimaData$bmi[pimaData$mass > 35] <- 1

logRegMulti <- glm(response~glucose + bmi, data=pimaData, family="binomial")
summary(logRegMulti)

```

### Part B

*Plot glucose levels versus diabetes (Y/N). Add the predicted probability of having diabetes as a function of glucose, for (i) individuals with BMI <35, and (ii) for individuals with BMI >35.*

```{r}
#Glucose for Low BMI
#       vector of 1s for intercept    vector of random glucose levels btw max and min        all bmi's > 35
BMILow <- cbind(rep(1,10000), seq(from=min(pimaData$glucose),to=max(pimaData$glucose),length.out=1000), rep(0, 10000))
lowPredxb <- BMILow%*%coef(logRegMulti)
lowPreds <-  1/(1+exp(-lowPredxb))
lowPredDF <- data.frame(x=BMILow[,2], preds=lowPreds)

#Glucose for High BMI
BMIHigh <- cbind(rep(1,10000), seq(from=min(pimaData$glucose),to=max(pimaData$glucose),length.out=1000), rep(1, 10000))
highPredxb <- BMIHigh%*%coef(logRegMulti)
highPreds <-  1/(1+exp(-highPredxb))
highPredDF <- data.frame(x=BMIHigh[,2], preds=highPreds)

ggplot() + 
  geom_point(data=pimaData,aes(y=response, x=glucose)) + 
  geom_line(data=lowPredDF,aes(x=x,y=lowPreds),color="Blue") +
  geom_line(data=highPredDF,aes(x=x,y=highPreds),color="Red") +
  labs(y="Probability of having Diabetes", x="Glucose") 
```

### Part C

*Add 95% confidence intervals for the predicted probability of having diabetes to your plots for both predictions in (b).*

```{r}
#CI Low BMI
lowSexb <- sqrt(diag(BMILow%*%vcov(logRegMulti)%*%t(BMILow)))
lowCixb <- cbind(lowPredxb-1.96*lowSexb,lowPredxb+1.96*lowSexb)
ciLow <- 1/(1+exp(-lowCixb))
lowPredDF <- lowPredDF %>% mutate(lb=ciLow[,1],ub=ciLow[,2])

#CI High BMI
highSexb <- sqrt(diag(BMIHigh%*%vcov(logRegMulti)%*%t(BMIHigh)))
highCixb <- cbind(highPredxb-1.96*highSexb,highPredxb+1.96*highSexb)
ciHigh <- 1/(1+exp(-highCixb))
highPredDF <- highPredDF %>% mutate(lb=ciHigh[,1],ub=ciHigh[,2])

ggplot() + 
  geom_point(data=pimaData,aes(y=response, x=glucose)) + 
  geom_line(data=lowPredDF,aes(x=x,y=lowPreds, color="BMI < 35")) +
  geom_ribbon(data=lowPredDF,aes(x=x,ymin=lb,ymax=ub),alpha=0.3,fill="Red") +
  geom_line(data=highPredDF,aes(x=x,y=highPreds, color="BMI > 35")) +
  geom_ribbon(data=highPredDF,aes(x=x,ymin=lb,ymax=ub),alpha=0.3,fill="Blue") +
  labs(y="Probability of having Diabetes", x="Glucose")
```


## Question 3: LDA

### Part A

*Split the data into training/testing sets, with 80% training. Fit a LDA model with diabetes (Y/N) as the response and glucose and BMI as covariates (use the actual BMI values this time.)*

```{r}
library(MASS)

set.seed(4)
train = sample(768, 768*.8)
train_set <- pimaData[train, ]
test_set <- pimaData[-train, ]

lda_fit <- lda(response ~ glucose + mass, data = pimaData,
    subset = train)
lda_fit
```

### Part B

*Plot glucose versus BMI and color the points by their true diabetes class (Y/N) for the test set.*

```{r}
ggplot(test_set, aes(x=glucose,y=mass)) + geom_point(aes(color = diabetes))
```


### Part C

*Add the LDA decision boundary for the model fit in (a).*
```{r}
Diabetic <- train_set$response == 1
NonDiabetic <- train_set$response == 0

Diabetic.df <- train_set[Diabetic, ]
NonDiabetic.df <- train_set[NonDiabetic, ]

x1 <- as.matrix(Diabetic.df[,c("glucose","mass")])
x2 <- as.matrix(NonDiabetic.df[,c("glucose","mass")])

pi1 = 0.6465798
pi2 = 0.3534202 

mu1hat <- colMeans(x1)
mu2hat <- colMeans(x2)
Sigmahat <- cov(rbind(x1,x2))
intercept <- c(log(pi1/pi2) - 0.5*t(mu1hat+mu2hat)%*%solve(Sigmahat)%*%(mu1hat-mu2hat))
slope <- solve(Sigmahat)%*%(mu1hat-mu2hat)

b0 <- intercept/(-slope[2])
b1 <- slope[1]/(-slope[2])

#Plotting
ggplot(test_set, aes(x=glucose,y=mass)) + geom_point(aes(color = diabetes)) +
  geom_abline(intercept=b0,slope=b1)


```

### Part D

*Print the 2x2 confusion matrix of the predictions on the test set versus their true values. Comment on the performance of LDA for predicting diabetes using glucose and BMI.*
```{r}
lda_pred <- predict(lda_fit, newdata = test_set)
temp_table <- table(lda_pred$class, test_set$response)
temp_table

#Calculating accuracy
mean(lda_pred$class == test_set$response)
```
The LDA's predictions are accurate almost 80% of the time. 

## Question 4: QDA

### Part A

*Using the same train/test split as in 2a, fit a QDA with the same variables.*
```{r}
qda_fit <- qda(response ~ glucose + mass, data = pimaData,
    subset = train)
qda_fit
```


### Part B

*Print the 2x2 confusion matrix of the predictions on the test set versus their true values.*
```{r}
# OLD CODE
#temp_table <- table(Wage$hi,round(predict(logReg,type='response')))
#temp_table

# NEW CODE
qda_pred <- predict(qda_fit, newdata = test_set)
temp_table2 <- table(qda_pred$class, test_set$response)
temp_table2
```

### Part C

*Compare the predictions of LDA versus QDA. Which model would you recommend in this case?*

```{r}
mean(qda_pred$class == test_set$response)
```
If we calculate accuracy for QDA: (TP + TN) / (TP + TN + FP + FN) = (28 + 94) / (28 + 94 + 23 + 9) =  0.7922078

If we calculate accuracy for LDA: (TP + TN) / (TP + TN + FP + FN) = (29 + 93) / (29 + 93 + 22 + 10) = 0.7922078

Both the LDA and QDA models show similar accuracy in predicting diabetes using BMI and glucose levels. We suggest choosing the LDA model because it is simpler and easier to understand. It has a straightforward structure and allows for better interpretation of its principles. The LDA model's simplicity makes it more practical and user-friendly, particularly when making predictions or explaining results. By opting for the LDA model, you can benefit from its strong predictive performance while enjoying the advantages of a simpler and more interpretable model.
