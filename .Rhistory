#  Calculate the AUC for the best threshold
best_auc <- auc(roc(College$Private, pred_probs, thresholds = best_threshold))
best_auc
# Extract the best threshold and corresponding AUC
best_threshold <- best_coords$threshold
best_threshold
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
```{r, warning=FALSE}
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
# Loading Carseat Data
library(ISLR2)
data("Carseats")
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
install.packages("MultBiplotR")
library(MultBiplotR)
library(MultBiplotR)
Protein
Q5df = Protein
Protein.df = Protein
plot.roc(College$Private, predict(logreg_model, type = "response"), legacy.axes = TRUE, percent = TRUE, xlab = "False Pos Percentage", ylab = "True Pos Percentage")
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y[index] ~ poly(X[index], 2))
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
bootstrap_predictions
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
predict(fit, newdata = data.frame(X = 1)
}
}
predict(fit, newdata = data.frame(X = 1)
)
B=200
df=data.frame(Y=Y,X=X)
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y ~ poly(X, 2),data = df[index,])
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
roc.info
roc.info$thresholds
head(roc.df)
Carseats
### ANOVA METHOD
library(dplyr)
data.subset = Carseats[, -c(2,3)]
lmfit1 = lm(Sales~., data=data.subset)
lmfit2 = lm(Sales~., data=Carseats)
anova(lmfit1,lmfit2)
summary_lm
B=200
df=data.frame(Y=Y,X=X)
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y ~ poly(X, 2),data = df[index,])
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
B=200
df=data.frame(Y=Y,X=X)
bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
index = sample(1:200, replace = T)
fit = lm(Y ~ poly(X, 2),data = df[index,])
bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}
lower_bound <- quantile(bootstrap_predictions, 0.05, na.rm=T)
upper_bound <- quantile(bootstrap_predictions, 0.95, na.rm=T)
lower_bound
upper_bound
?quantile
# Calculating RMSE Root Mean Square Error
sqrt(mean((ridge.pred - y.test)^2))
df=data.frame(Y=Y,X=X)
# Set the number of bootstrap samples
B <- 200
# Vector to store bootstrap estimates
bootstrap_estimates <- numeric(B)
#Bootstrp Procedure
for (b in 1:B) {
# Sample with replacement from the original dataset
bootstrap_data <- df[sample(nrow(df), replace = TRUE), ]
# Fit the model on the bootstrap sample
fit <- lm(Y ~ poly(X, 2), data = bootstrap_data)
# Predict the response at X = 1
bootstrap_estimates[b] <- predict(fit, newdata = data.frame(X = 1))
}
confidence_interval <- quantile(bootstrap_estimates, c(0.05, 0.95))
confidence_interval
# Fit the linear model
lm.fit <- lm(Sales ~ CompPrice + Income, data = Carseats)
# Get the summary of the model
summary_lm <- summary(lm.fit)
# Extract the p-values
p_values <- summary_lm$coefficients[, "Pr(>|t|)"]
# Print the p-values
print(p_values)
# Perform Bonferroni and Bonferroni-Holm Coerrection
bonf <- p.adjust(p_values,method="bonferroni")
bh <- p.adjust(p_values,method="BH")
print(bh)
ridge.mod$beta
coef(ridge.mod)
?train
View(Carseats)
# Loading Carseat Data
library(ISLR2)
data("Carseats")
# Fit Linear Regression Model
lm.fit <- lm(Sales ~ ., data = Carseats)
# Report Coefficient Estimates
summary(lm.fit)
coefficients(lm.fit)
# Linear Model Assumption Testing
par(mfrow = c(2, 2))
plot(lm.fit)
# Fit the linear model
lm.fit <- lm(Sales ~ CompPrice + Income, data = Carseats)
# Get the summary of the model
summary_lm <- summary(lm.fit)
# Extract the p-values
p_values <- summary_lm$coefficients[, "Pr(>|t|)"]
# Print the p-values
print(p_values)
# Perform Bonferroni and Bonferroni-Holm Coerrection
bonf <- p.adjust(p_values,method="bonferroni")
bh <- p.adjust(p_values,method="BH")
print(bh)
### ANOVA METHOD
library(dplyr)
data.subset = Carseats[, -c(2,3)]
lmfit1 = lm(Sales~., data=data.subset)
lmfit2 = lm(Sales~., data=Carseats)
anova(lmfit1,lmfit2)
# Define Variables
x <- model.matrix(Sales ~ ., Carseats)[, -1]
y <- Carseats$Sales
# Train/Validation Split
set.seed(1)
train <- sample(1:nrow(x), nrow(x) *0.8)
test <- (-train)
y.test <- y[test]
library(glmnet)
rr.fit <- cv.glmnet(x[train, ], y[train], alpha = 0, nfolds = 10)
# Report best lambda
bestLambda <- rr.fit$lambda.min
bestLambda
#Run ridge regression with best lambda, report coefficients
ridge.mod <- glmnet(x, y, alpha = 0, lambda = bestLambda)
coef(ridge.mod)
# Predict values using new X's
ridge.pred <- predict(ridge.mod, s = bestLambda, newx = x[test, ])
# Calculating RMSE Root Mean Square Error
sqrt(mean((ridge.pred - y.test)^2))
#fit using training
library(caret)
library(randomForest)
set.seed(1)
rf.carseats <- randomForest(Sales ~ ., data = Carseats, subset = train, mtry = 4, importance = TRUE)
rf.predict <- predict(rf.carseats, newdata = Carseats[test, ])
#get RMSE using test
sqrt(mean((rf.predict - y.test)^2))
set.seed(1)
X <- rt(n=200, df=15)
sorted_idx = order(X)
X = X[sorted_idx]
epsilon <- rt(n=200, df=5)
Y <- 5+2*sin(X)-(7*(exp(2*cos(X))/(1+exp(2*cos(X))))) + epsilon
plot(X, Y)
colors <- sample(colors(), 5)
legend_labels <- c()
for (p in 1:5) {
fit <- lm(Y ~ poly(X, p))
val <- predict(fit, data.frame(X = X))
lines(X, val, col = colors[p], lwd = 2)
legend_labels <- c(legend_labels, paste0("Degree ", p))
}
legend("topright", legend = legend_labels, col = colors, lwd = 2)
library(boot)
set.seed (1)
df = cbind.data.frame(X,Y)
predErr <- rep (0, 5)
for (i in 1:5) {
glm.fit <- glm(Y~poly(X,i), data = df)
predErr[i] <- cv.glm (df , glm.fit , K = 10)$delta[1]
}
predErr
min(predErr)
fit = lm(Y ~ poly(X,2))
ci = predict(fit, data.frame(X=1), level = 0.90, interval = "confidence")
ci
df=data.frame(Y=Y,X=X)
# Set the number of bootstrap samples
B <- 200
# Vector to store bootstrap estimates
bootstrap_estimates <- numeric(B)
#Bootstrp Procedure
for (b in 1:B) {
# Sample with replacement from the original dataset
bootstrap_data <- df[sample(nrow(df), replace = TRUE), ]
# Fit the model on the bootstrap sample
fit <- lm(Y ~ poly(X, 2), data = bootstrap_data)
# Predict the response at X = 1
bootstrap_estimates[b] <- predict(fit, newdata = data.frame(X = 1))
}
confidence_interval <- quantile(bootstrap_estimates, c(0.05, 0.95))
confidence_interval
data(College)
set.seed(4)
train = sample(nrow(College), nrow(College)*.8)
train_set <- College[train, ]
valid_set <- College[-train, ]
logRegMulti <- glm(Private ~ ., data=College, family="binomial")
summary(logRegMulti)
exp(coefficients(logRegMulti))
# Set the number of folds for cross-validation
num_folds <- 10
accuracy <- numeric(num_folds)
set.seed(123)  # For reproducibility
fold_indices <- sample(rep(1:num_folds, length.out = nrow(College)))
# Perform cross-validation
for (i in 1:num_folds) {
# Split the data into training and test sets based on the fold indices
train_data <- College[fold_indices != i, ]
test_data <- College[fold_indices == i, ]
# Fit logistic regression model on training data
logreg_model <- glm(Private ~ ., data = train_data, family = "binomial")
# Predict on test data
test_preds <- predict(logreg_model, newdata = test_data, type = "response")
test_class_preds <- ifelse(test_preds > 0.96, "Yes", "No")
accuracy[i] <- mean(test_class_preds == test_data$Private)
}
#average test error
test_error <- 1 - mean(accuracy)
print(test_error)
library(pROC)
# Fit logistic regression model on the entire dataset
logreg_model <- glm(Private ~ ., data = College, family = "binomial")
pred_probs <- predict(logreg_model, type = "response")
# Create a ROC object
roc.info <- roc(College$Private, pred_probs, legacy.axes = TRUE)
roc.df <- data.frame(tpp = roc.info$sensitivities*100, fpp = (1 - roc.info$specificities)*100, thresholds = roc.info$thresholds)
# Get the coordinates of the point with the highest AUC
best_coords <- coords(roc.info, "best")
# Extract the best threshold and corresponding AUC
best_threshold <- best_coords$threshold
best_threshold
#  Testing the AUC for the best threshold
best_auc <- auc(roc(College$Private, pred_probs, thresholds = best_threshold))
plot.roc(College$Private, predict(logreg_model, type = "response"), legacy.axes = TRUE, percent = TRUE, xlab = "False Pos Percentage", ylab = "True Pos Percentage")
######################
# Fit logistic regression model on the entire dataset
logreg_model <- glm(Private ~ ., data = College, family = "binomial")
pred_probs <- predict(logreg_model, type = "response")
?coords
# Get the coordinates of the point with the highest AUC
best_coords <- coords(roc.info, "closest.topleft")
# Get the coordinates of the point with the highest AUC
best_coords <- coords(roc.info, "all")
View(best_coords)
# Get the coordinates of the point with the highest AUC
best_coords <- coords(roc.info, "best")
View(best_coords)
View(best_coords)
# Get the coordinates of the point with the highest AUC
best_coords <- coords(roc.info, "local maximas")
View(best_coords)
View(best_coords)
# Get the coordinates of the point with the highest AUC
best_coords <- coords(roc.info, "best")
# Extract the best threshold and corresponding AUC
best_threshold <- best_coords$threshold
best_threshold
test_class_preds <- ifelse(test_preds > 0.65, "Yes", "No")
accuracy[i] <- mean(test_class_preds == test_data$Private)
# Perform cross-validation
for (i in 1:num_folds) {
# Split the data into training and test sets based on the fold indices
train_data <- College[fold_indices != i, ]
test_data <- College[fold_indices == i, ]
# Fit logistic regression model on training data
logreg_model <- glm(Private ~ ., data = train_data, family = "binomial")
# Predict on test data
test_preds <- predict(logreg_model, newdata = test_data, type = "response")
test_class_preds <- ifelse(test_preds > 0.65, "Yes", "No")
accuracy[i] <- mean(test_class_preds == test_data$Private)
}
#average test error
test_error <- 1 - mean(accuracy)
print(test_error)
library(pROC)
# Fit logistic regression model on the entire dataset
logreg_model <- glm(Private ~ ., data = College, family = "binomial")
pred_probs <- predict(logreg_model, type = "response")
# Create a ROC object
roc.info <- roc(College$Private, pred_probs, legacy.axes = TRUE)
roc.df <- data.frame(tpp = roc.info$sensitivities*100, fpp = (1 - roc.info$specificities)*100, thresholds = roc.info$thresholds)
# Get the coordinates of the point with the highest AUC
best_coords <- coords(roc.info, "best")
# Extract the best threshold and corresponding AUC
best_threshold <- best_coords$threshold
best_threshold
plot.roc(College$Private, predict(logreg_model, type = "response"), legacy.axes = TRUE, percent = TRUE, xlab = "False Pos Percentage", ylab = "True Pos Percentage")
######################
```
plot.roc(College$Private, predict(logreg_model, type = "response"), legacy.axes = TRUE, percent = TRUE, xlab = "False Pos Percentage", ylab = "True Pos Percentage")
# Set the number of folds for cross-validation
num_folds <- 10
accuracy <- numeric(num_folds)
set.seed(123)  # For reproducibility
fold_indices <- sample(rep(1:num_folds, length.out = nrow(College)))
# Perform cross-validation
for (i in 1:num_folds) {
# Split the data into training and test sets based on the fold indices
train_data <- College[fold_indices != i, ]
test_data <- College[fold_indices == i, ]
# Fit logistic regression model on training data
logreg_model <- glm(Private ~ ., data = train_data, family = "binomial")
# Predict on test data
test_preds <- predict(logreg_model, newdata = test_data, type = "response")
test_class_preds <- ifelse(test_preds > 0.65, "Yes", "No")
accuracy[i] <- mean(test_class_preds == test_data$Private)
}
#average test error
test_error <- 1 - mean(accuracy)
print(test_error)
library(pROC)
# Fit logistic regression model on the entire dataset
logreg_model <- glm(Private ~ ., data = College, family = "binomial")
pred_probs <- predict(logreg_model, type = "response")
# Create a ROC object
roc.info <- roc(College$Private, pred_probs, legacy.axes = TRUE)
roc.df <- data.frame(tpp = roc.info$sensitivities*100, fpp = (1 - roc.info$specificities)*100, thresholds = roc.info$thresholds)
# Get the coordinates of the point with the highest AUC
best_coords <- coords(roc.info, "best")
# Extract the best threshold and corresponding AUC
best_threshold <- best_coords$threshold
best_threshold
plot.roc(College$Private, predict(logreg_model, type = "response"), legacy.axes = TRUE, percent = TRUE, xlab = "False Pos Percentage", ylab = "True Pos Percentage")
######################
