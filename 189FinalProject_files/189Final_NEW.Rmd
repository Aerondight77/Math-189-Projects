---
title: "Math189 Final Project"
author: "Daria Barbour-Brown"
date: "2023-05-25"
output:
  html_document: default
  pdf_document: default
---
# SEE PROFESSOR
- Question 1, part C
- Question 2, part D
- Q 3 part E !!!
- Validating vs Test set??





### Application Problems
#### 1. Consider the Carseats data in the ISLR2 package.
##### (a) Fit a linear regression model with Sales as the response and all other variables as covariates. Report the coefficient estimates.

```{r}
# Loading Carseat Data
library(ISLR2)
data("Carseats")
```

```{r}
# Fit Linear Regression Model
lm.fit <- lm(Sales ~ ., data = Carseats)

# Report Coefficient Estimates
summary(lm.fit)
coefficients(lm.fit)

```


##### (b) Determine whether the linear model is appropriate.
```{r} 
# Linear Model Assumption Testing
par(mfrow = c(2, 2))
plot(lm.fit)

```
We can determine the fitness of applying a linear model to our data by observing the residual plots in order to test the assumptions for linear regression. In our the above visualization, we plot our residuals against the fitted values. In a residual plot for a linear model, you typically look for certain patterns or characteristics that can help assess the adequacy of the model. The spread of the residuals should remain roughly constant across the range of predicted values. The residuals should appear randomly scattered around the horizontal axis.  The residuals should be independent of each other, meaning that the value of one residual should not provide any information about the value of another residual. The residuals should not exhibit any systematic curvature or nonlinear patterns. In our case, we see that none of these conditions are violated, meaning that the data points do not seem to show any systematic deviations from randomness.   

Next we consider the Normal Q-Q plot. In a normal Q-Q plot, the residuals are plotted against the quantiles of a theoretical normal distribution. If the residuals are normally distributed, the points on the plot should roughly follow a straight line. Departures from normality can impact the accuracy of statistical tests and confidence intervals associated with the model. In our case,  our normality assumption is met, given that our residuals follow the line denoting the quantiles of a theoretical normal distribution. We conclude that fitting a linear model is indeed appropriate.


##### (c) Let $\beta_1$ and $\beta_2$ be the coefficients for CompPrice and Income, respectively. Test the hypothesis that $\beta_1$=$\beta_2$=0. State your hypothesis, test statistic, and test statistic’s distribution clearly. Choose an $\alpha$ you feel is appropriate.

```{r}
# Fit the linear model
lm.fit <- lm(Sales ~ CompPrice + Income, data = Carseats)

# Get the summary of the model
summary_lm <- summary(lm.fit)

# Extract the p-values
p_values <- summary_lm$coefficients[, "Pr(>|t|)"]

# Print the p-values
print(p_values)

# Perform Bonferroni and Bonferroni-Holm Coerrection
bonf <- p.adjust(p_values,method="bonferroni")
bh <- p.adjust(p_values,method="BH")

print(bh)
```
Null hypothesis: There is no linear relationship between the predictor variables and the response variable. In other words, the coefficients for both CompPrice and Income are equal to zero.

Alternative hypothesis: There is a linear relationship between the predictor variables and the response variable. At least one of the coefficients for CompPrice or Income is not equal to zero.

The test statistics for each coefficient are represented by their respective t-values. Meaning that our test statistic for CompPrice is 1.548, and 3.187 for Income.

The distribution of the test statistics is the t-distribution, which is used in hypothesis testing for linear regression. 

Since we are testing multiple hypotheses, we need to account for the family-wise error rate. We do so by applying the Bonferroni-Holm corrrection to our p-values. 

Our resulting p-values are 0.1222 and 0.0023, for Carprice and Income, respectively. Comparing our adjusted p-values to 95% significance level, $\alpha$=0.05, we can conclude that we should reject our null hypothesis that both covariates are jointly equal to zero. 


#### 2. Consider the Carseats data again.
##### (a) Split the data into a training set and a validation set. State the proportions of your training/validation split.
```{r}
# Define Variables
x <- model.matrix(Sales ~ ., Carseats)[, -1]
y <- Carseats$Sales

# Train/Validation Split
set.seed(1)
train <- sample(1:nrow(x), nrow(x) *0.8)
test <- (-train)
y.test <- y[test]
```
We will train our model ou 80 percent of our data. The validation(test) set will be comprised of the remaining 20 percent of unseen data.

##### (b)Fit a ridge regression model on the training data, choosing the "lambda" $\lambda$ by cross-validation and reporting the final coefficients. Choose an appropriate value for K when doing cross-validation.
```{r}
library(glmnet)
rr.fit <- cv.glmnet(x[train, ], y[train], alpha = 0, nfolds = 10) 
plot(rr.fit)
```

```{r}
# Report best lambda
bestLambda <- rr.fit$lambda.min
bestLambda

#Run ridge regression with best lambda, report coefficients
ridge.mod <- glmnet(x, y, alpha = 0, lambda = bestLambda)
ridge.mod$beta
```

##### (c) Report the RMSE using the validation set on the model from 2b.
```{r}
# Predict values using new X's
ridge.pred <- predict(ridge.mod, s = bestLambda, newx = x[test, ])

# Calculating RMSE Root Mean Square Error
sqrt(mean((ridge.pred - y.test)^2))
```


##### (d) Fit a random forest model on the training data, and report the RMSE on the validation set.
```{r}
#fit using training 
library(caret)
library(randomForest)
set.seed(1)

rf.carseats <- randomForest(Sales ~ ., data = Carseats, subset = train, mtry = 4, importance = TRUE)  
rf.predict <- predict(rf.carseats, newdata = Carseats[test, ])

#get RMSE using test
sqrt(mean((rf.predict - y.test)^2))
```

##### (e) For both of the models you fit in (b) and (d), give an example why a marketing team would prefer one model over the other.
  A marketing team might prefer to use the Random Forest model if they are presenting to higher-ups in the company or some other "non-experts". This is because trees are quite easy to explain and mimic the natural human decision-making process. Random forests also give an easily understood estimate about which features in the model have the most significant impact on the target variable, Sales. This could be valuable for marketing teams to make data-driven decisions while providing nice illustrations to back their findings.  
  
  Ridge regression estimates the impact of each feature on the target variable through its coefficients. This could be valuable in marketing when it is necessary to communicate the specific influence of different variables on Sales. Ridge regression shrinks the coefficients of less important features towards zero which might be advantageous when dealing with a large number of predictors. 



#### 3. In this question, you will simulate data to perform regression between X and Y.
##### (a) Use the rt() function to generate a predictor X of length n=200. Set df=15 for X
```{r}
set.seed(1)
X <- rt(n=200, df=15)
sorted_idx = order(X)
X = X[sorted_idx]
```

##### (b) Use rt() to generate a noise vector $\epsilon$. Set df=5.
```{r}
epsilon <- rt(n=200, df=5)
```

##### (c) Generate a response vector Y of length n according to: $Y=5+2sin(X)− \frac{(7*\exp(2*cos(X)}{1+\exp(2*cos(X)} +\epsilon$
```{r}
Y <- 5+2*sin(X)-(7*(exp(2*cos(X))/(1+exp(2*cos(X))))) + epsilon

```

##### (d) Fit polynomial regression for Y on X with the order of X ranging from 1 to 5. (i.e.Y=β0+β1X+ϵ,Y=β0+β1X+β2X2+ϵ,...,Y=β0+β1X+β2X2β3X3+β4X4+β5X5+ϵ) and plot each of the five model fits, in different colors and with a legend, on top of your simulated data.
```{r}

plot(X, Y)
colors <- sample(colors(), 5)
legend_labels <- c()

for (p in 1:5) {
  fit <- lm(Y ~ poly(X, p))
  val <- predict(fit, data.frame(X = X))
  lines(X, val, col = colors[p], lwd = 2)
  legend_labels <- c(legend_labels, paste0("Degree ", p))
}

legend("topright", legend = legend_labels, col = colors, lwd = 2)
```


##### (e) Which one of these models do you prefer? Justify your answer.
We will perform K-fold cross validation to decide which polynomial fits our data best. We use K=10.
```{r}
library(boot)
set.seed (1)
df = cbind.data.frame(X,Y)
predErr <- rep (0, 5)
for (i in 1:5) {
  glm.fit <- glm(Y~poly(X,i), data = df)
  predErr[i] <- cv.glm (df , glm.fit , K = 10)$delta[1]
}
predErr
min(predErr)
```
Using cross validation we find that the model with the smallest validation/prediction error is the final polynomial with degree 5 which agrees with the above plot.


##### (f) For the model Y=β0+β1X+β2X2+ϵ, compute a 90% confidence interval at X=1 using least squares theory. Provide an interpretation for this interval.
```{r}
fit = lm(Y ~ poly(X,2))
ci = predict(fit, data.frame(X=1), level = 0.90, interval = "confidence")
ci
```
90% CI: [0.6640983, 1.141839]

This CI says that if we were to repeat the data collection and modeling process multiple times, we would expect the true value of the response variable (when X=1) to fall within this interval in approximately 90% of the cases.


##### (g) For the model Y=β0+β1X+β2X2+ϵ, compute a 90% confidence interval at X=1 using a bootstrap. Provide an interpretation for this interval.

```{r}
B=200

bootstrap_predictions <- numeric(B)
for(b in 1:B)
{
  index = sample(1:200, replace = T)
  fit = lm(Y[index] ~ poly(X[index], 2))
  bootstrap_predictions[b] <- predict(fit, newdata = data.frame(X = 1))
}

lower_bound <- quantile(bootstrap_predictions, 0.05)
upper_bound <- quantile(bootstrap_predictions, 0.95)

lower_bound
upper_bound
```
#### Need to give interpretation!!!
****** NEED TO PROVIDE INTERPRETATION 

#### 4. Consider the College data set in the ISLR2 package.
##### (a) Split the data set into a training and validation set.
```{r}
data(College)
```


```{r}
set.seed(4)
train = sample(nrow(College), nrow(College)*.8)
train_set <- College[train, ]
valid_set <- College[-train, ]

```

##### (b) Perform logistic regression on the training data to predict the variable Private using all other variables. Provide an interpretation of the coefficient for Top10Perc.
```{r}
logRegMulti <- glm(Private ~ ., data=College, family="binomial")
summary(logRegMulti)
exp(coefficients(logRegMulti))
```
$\beta_4$ = 8.451e-03 corresponding to Top10perc

A positive coefficient indicates that an increase in the feature's value is associated with a increase in the log-odds of the target variable Private. We take the exponential of the the coefficients to get odds of our response... In this case, as a college increases by 1 unit towards a top-10 rating, the odds of it being a Private school increases by .7%.  


##### (c) What is the test error for the logistic regression (justify your selection of your threshold)?

```{r}
# Set the number of folds for cross-validation
num_folds <- 10

accuracy <- numeric(num_folds)
set.seed(123)  # For reproducibility
fold_indices <- sample(rep(1:num_folds, length.out = nrow(College)))

# Perform cross-validation
for (i in 1:num_folds) {
  # Split the data into training and test sets based on the fold indices
  train_data <- College[fold_indices != i, ]
  test_data <- College[fold_indices == i, ]
  
  # Fit logistic regression model on training data
  logreg_model <- glm(Private ~ ., data = train_data, family = "binomial")
  
  # Predict on test data
  test_preds <- predict(logreg_model, newdata = test_data, type = "response")
  test_class_preds <- ifelse(test_preds > 0.96, "Yes", "No")
  
  accuracy[i] <- mean(test_class_preds == test_data$Private)
}

#average test error
test_error <- 1 - mean(accuracy)
print(test_error)

#################### need to justify selection of threshold!?
library(pROC) # ROC PLOT NEED HELP

# Fit logistic regression model on the entire dataset
logreg_model <- glm(Private ~ ., data = College, family = "binomial")

pred_probs <- predict(logreg_model, type = "response")

# Create a ROC object
roc.info <- roc(College$Private, pred_probs, legacy.axes = TRUE)
roc.df <- data.frame(tpp = roc.info$sensitivities*100, fpp = (1 - roc.info$specificities)*100, thresholds = roc.info$thresholds)

#to find best threshold
roc.df[roc.df$tpp > 77 & roc.df$tpp < 83, ]


plot.roc(College$Private, predict(logreg_model, type = "response"), legacy.axes = TRUE, percent = TRUE, xlab = "False Pos Percentage", ylab = "True Pos Percentage")

######################
```
The best choice of threshold depends on the goal we are trying to accomplish. Predicting whether or not a school is Private or not, is not necessarily a matter of life or death. But, if I was looking to pick a school to send my child, and bad prediction could result in higher costs perhaps. If we go by the ROC curve, we are trying to find the balance between specificity and sensitivity (false positives and true positives). Looking at the ROC curve, we filter for ttp between 77% and 83% which gives us a threshold of about .96. This results is a larger prediction error than when using .5 as a threshold but can assure us of a better balance between false positive and false negative rates. 


##### (d) Fit an LDA to the same model, and report the test error.
```{r}
library(MASS)
num_folds <- 10

accuracy <- numeric(num_folds)

set.seed(123) 
fold_indices <- sample(rep(1:num_folds, length.out = nrow(College)))

#cross-validation
for (i in 1:num_folds) {
  train_data <- College[fold_indices != i, ]
  test_data <- College[fold_indices == i, ]
  
  # Fit LDA model on training data
  lda_model <- lda(Private ~ ., data = train_data)
  
  # Predict on test data
  test_preds <- predict(lda_model, newdata = test_data)
  test_class_preds <- test_preds$class
  
  accuracy[i] <- mean(test_class_preds == test_data$Private)
}

test_error <- 1 - mean(accuracy)
print(test_error)

```


##### (e) Fit an QDA to the same model, and report the test error.
```{r}
library(MASS)
num_folds <- 10
accuracy <- numeric(num_folds)

set.seed(123) 
fold_indices <- sample(rep(1:num_folds, length.out = nrow(College)))

for (i in 1:num_folds) {
  train_data <- College[fold_indices != i, ]
  test_data <- College[fold_indices == i, ]
  
  # Fit QDA model on training data
  qda_model <- qda(Private ~ ., data = train_data)
  
  # Predict on test data
  test_preds <- predict(qda_model, newdata = test_data)
  test_class_preds <- test_preds$class

  accuracy[i] <- mean(test_class_preds == test_data$Private)
}

test_error <- 1 - mean(accuracy)
print(test_error)

```


##### (f) Fit an SVM to the same model, and report the test error.
```{r}
library(e1071)
num_folds <- 10
accuracy <- numeric(num_folds)

set.seed(123) 
fold_indices <- sample(rep(1:num_folds, length.out = nrow(College)))

for (i in 1:num_folds) {
  train_data <- College[fold_indices != i, ]
  test_data <- College[fold_indices == i, ]
  
  # Fit SVM model on training data
  svm_model <- svm(Private ~ ., data = train_data)
  
  # Predict on test data
  test_preds <- predict(svm_model, newdata = test_data)

  accuracy[i] <- mean(test_preds == test_data$Private)
}
test_error <- 1 - mean(accuracy)
print(test_error)

```


##### (g) Pick which model you think is the best and explain your choice.
Logistic Regression has the lowest prediction error, therefor it is the best choice of model to predict whether a school is Private or not. LDA is hugely sensitive to non-gaussian data so it makes sense that this would not be a good choice. 


#### 5. For this problem use the protein.csv file which contains protein consumption in twenty-five European countries for nine food groups. It is available in the MultBiplotR R package.
##### (a) Perform principal component analysis on these data (omitting variables Comunist and Region). Report the proportion of variance and cumulative proportion of variance explained by the first 5 principal components.
```{r}
protein <- read.csv("protein.csv")
protein <- protein[, -1]
protein <- protein[, -1]
pca <- prcomp(protein, scale = TRUE)
```

Proportion of variance:
```{r}
pov <- (pca$sdev^2) / sum(pca$sdev^2)
pov
```

Cumulative proportion of variance explained by the first 5 principal components:
```{r}
cpov <- cumsum(pov)[1:5]
cpov
```



##### (b) Provide an interpretation of the first two principal components.
```{r}
pca$rotation
```
Principal Component 1 places the largest weights on Eggs, Cereal, and Nuts. This principal component basically describes the overall rates of proteins. These three features account for the most variation in the data, specifically 44.5% of the variation in the data, so they are given the larger magnitudes in the first principal component. 

Principal Component 2 places the largest weights on fish and fruits/vegetables, and less on the rest of the features. PC2

##### (c) Create a biplot for the first two principal components. Based on this plot, which variable(s) is Milk most correlated with? Which variable(s) is Milk most negatively correlated with? Which variables is Milk uncorrelated with?

```{r fig.width=15, fig.height=15}
biplot(pca, scale = 0, expand=1, cex=1, xlim=c(-4,4), ylim=c(-2,4))
```
Based on this plot, `Milk` is most correlated with `White Meat`, `Red_Meat`, and `Eggs`, and is most negatively correlated with `Nuts`. `Milk` is uncorrelated with `Starch`, `Fish`, `Fruits_Vegetables`, and `Cereal`.

##### (d) Comment on the differences between countries in the North Region and Central Region using only the first two principal components and the respective interpretations of those principal components.

Observations corresponding to the North and Central Region:

* North: 6, 8, 15, 20
* Central: 2, 3, 5, 7, 9, 11, 12, 14, 16, 21, 22, 23, 24

North => more meat, animal-based food (in negative PC1 and PC2 region)
Central => 

### Conceptual Problems
#### 6. Explain why the bootstrap may be more beneficial for random forest than it would be for linear regression.

  Random forest models can handle high-dimensional data where linear regression may suffer from overfitting or high variance in the case where number of observations is smaller than the number of features. Using a bootstrap sample in random forests helps to mitigate this issue of overfitting by creating subsets of the data during each tree's construction.

******* ??? Random forest is a non-parametric algorithm capable of capturing nonlinear relationships and interactions between variables. Linear regression assumes a linear relationship between predictors and the response, which may not adequately capture complex patterns present in the data. The bootstrap sampling used in random forests allows each tree to learn different aspects of the data, enabling the model to capture nonlinearity and interactions more effectively.

  In the case where our features have high correlation, linear regression models may return unreliable coefficient estimates. Random forests can overcome this correlation problem randomly subsets of predictors at each split, reducing the impact of highly correlated variables. Bootstrapping further enhances this capability by sampling from the original data set (with replacement) and thus creating diverse subsets of the data. 

#### 7. Give an example of a scenario where you test multiple hypotheses but would not want to corect for FEWR or FDR.

  We may want to abstain from correcting for FWER or FDR if we are in the initial stages of a data research project. At this stage of data science, we many simply be looking to identify patterns in the data and may not yet have concrete hypotheses we are interested in testing. During exploratory data analysis it is our job to test multiple hypotheses and then decide which of them we are most interested in confirming or rejecting. At this point it is not our goal to restrict error rates since doing so could also restrict our ability to fully explore the data. It does not make sense to confine hypotheses conclusions such as Type-1/2 error when we have not yet established the specific claims we wish to test. 


#### 8. Why is it necessary to be aware of a model’s assumptions, and check those assumptions before using the trained model for inference or prediction?

  Violating model assumptions can lead to biased parameter estimates and incorrect hypothesis tests and confidence intervals. When it comes to inference and prediction, validating these assumptions assures we can construct a reliable model to fit our data. The prediction accuracy of a model may suffer if it's assumptions are not met. 
  





